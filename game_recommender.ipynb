{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2365738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6417106, 5)\n",
      "\n",
      "Column info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6417106 entries, 0 to 6417105\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   app_id        int64 \n",
      " 1   app_name      object\n",
      " 2   review_text   object\n",
      " 3   review_score  int64 \n",
      " 4   review_votes  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 244.8+ MB\n",
      "None\n",
      "\n",
      "Sample data:\n",
      "   app_id        app_name                                        review_text  \\\n",
      "0      10  Counter-Strike                                    Ruined my life.   \n",
      "1      10  Counter-Strike  This will be more of a ''my experience with th...   \n",
      "2      10  Counter-Strike                      This game saved my virginity.   \n",
      "3      10  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
      "4      10  Counter-Strike           Easy to learn, hard to master.             \n",
      "\n",
      "   review_score  review_votes  \n",
      "0             1             0  \n",
      "1             1             1  \n",
      "2             1             0  \n",
      "3             1             0  \n",
      "4             1             1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv', low_memory=False)\n",
    "\n",
    "# Display basic shape and column info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Show first 5 rows\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb614a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping full duplicates: (4621004, 5)\n",
      "After dropping rows with null review_text or app_name: (4483850, 5)\n",
      "After filtering short reviews: (4437086, 5)\n",
      "\n",
      "Review score distribution:\n",
      " review_score\n",
      " 1    3642297\n",
      "-1     794789\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample after cleaning:\n",
      "         app_name                                        review_text  \\\n",
      "0  Counter-Strike                                    Ruined my life.   \n",
      "1  Counter-Strike  This will be more of a ''my experience with th...   \n",
      "2  Counter-Strike                      This game saved my virginity.   \n",
      "3  Counter-Strike  • Do you like original games? • Do you like ga...   \n",
      "4  Counter-Strike           Easy to learn, hard to master.             \n",
      "\n",
      "   review_score  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "# Drop full duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "print(\"After dropping full duplicates:\", df.shape)\n",
    "\n",
    "# Drop rows with missing review text or app name\n",
    "df = df.dropna(subset=['review_text', 'app_name'])\n",
    "print(\"After dropping rows with null review_text or app_name:\", df.shape)\n",
    "\n",
    "# Remove reviews with very short length (less than 10 characters after stripping)\n",
    "df['review_text'] = df['review_text'].astype(str)\n",
    "df = df[df['review_text'].str.strip().str.len() >= 10]\n",
    "print(\"After filtering short reviews:\", df.shape)\n",
    "\n",
    "# Review score distribution\n",
    "print(\"\\nReview score distribution:\\n\", df['review_score'].value_counts())\n",
    "\n",
    "# Show sample after cleaning\n",
    "print(\"\\nSample after cleaning:\")\n",
    "print(df[['app_name', 'review_text', 'review_score']].head())\n",
    "\n",
    "# Store for future steps\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd24fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering to popular games (≥ 50 reviews): (4355551, 5)\n",
      "\n",
      "Top 5 most reviewed games:\n",
      "app_name\n",
      "Terraria     77598\n",
      "PAYDAY 2     62932\n",
      "Dota 2       48949\n",
      "Undertale    48193\n",
      "Warframe     44459\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter games with at least 50 reviews\n",
    "\n",
    "# Count number of reviews per game\n",
    "review_counts = df_clean['app_name'].value_counts()\n",
    "\n",
    "# Filter games with at least 50 reviews\n",
    "popular_games = review_counts[review_counts >= 50].index\n",
    "df_filtered = df_clean[df_clean['app_name'].isin(popular_games)]\n",
    "\n",
    "print(\"After filtering to popular games (≥ 50 reviews):\", df_filtered.shape)\n",
    "\n",
    "# Check most reviewed games\n",
    "print(\"\\nTop 5 most reviewed games:\")\n",
    "print(df_filtered['app_name'].value_counts().head())\n",
    "\n",
    "# Store for next step\n",
    "df_clean = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d2acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review score distribution after filtering:\n",
      "review_score\n",
      " 1    3586594\n",
      "-1     768957\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample cleaned reviews:\n",
      "                                               review_text  \\\n",
      "1564255  dont buy this game or buy its dlc's. why? you ...   \n",
      "3136233  A beautiful game of a majestic and partially ♥...   \n",
      "4578296  Simply great game. One of the best, but Arkham...   \n",
      "6028133          i recommend league of legends to everyone   \n",
      "3537423         This game is fantastic. Worth every penny.   \n",
      "\n",
      "                                                clean_text  \n",
      "1564255  dont buy this game or buy its dlcs why you can...  \n",
      "3136233  a beautiful game of a majestic and partially g...  \n",
      "4578296  simply great game one of the best but arkham c...  \n",
      "6028133          i recommend league of legends to everyone  \n",
      "3537423           this game is fantastic worth every penny  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Keep only review_score 1 (positive) and -1 (negative)\n",
    "df_clean = df_clean[df_clean['review_score'].isin([1, -1])]\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_review(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df_clean['clean_text'] = df_clean['review_text'].astype(str).apply(clean_review)\n",
    "\n",
    "# Confirm result\n",
    "print(\"Review score distribution after filtering:\")\n",
    "print(df_clean['review_score'].value_counts())\n",
    "\n",
    "print(\"\\nSample cleaned reviews:\")\n",
    "print(df_clean[['review_text', 'clean_text']].sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede512d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced review score distribution:\n",
      "review_score\n",
      " 1    768957\n",
      "-1    768957\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n",
      "   review_score                                         clean_text\n",
      "0             1  oh god i cant say anything wrong against this ...\n",
      "1             1  took me about lives to do the first level anno...\n",
      "2            -1         brothers a tale of no fun this movie sucks\n",
      "3            -1  i actually was kind of expecting a similar sty...\n",
      "4            -1  its ok if flying in the air and futuristic are...\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset (same number of positive and negative reviews)\n",
    "\n",
    "# Sample 768957 positive reviews (to match negative)\n",
    "df_pos = df_clean[df_clean['review_score'] == 1].sample(768957, random_state=42)\n",
    "df_neg = df_clean[df_clean['review_score'] == -1]\n",
    "\n",
    "# Concatenate and shuffle\n",
    "df_balanced = pd.concat([df_pos, df_neg], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Confirm shape\n",
    "print(\"Balanced review score distribution:\")\n",
    "print(df_balanced['review_score'].value_counts())\n",
    "\n",
    "# Show some samples\n",
    "print(\"\\nSample rows:\")\n",
    "print(df_balanced[['review_score', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4931fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1230331\n",
      "Test set size: 307583\n",
      "\n",
      "Label distribution in training set:\n",
      "review_score\n",
      " 1    615166\n",
      "-1    615165\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution in test set:\n",
      "review_score\n",
      "-1    153792\n",
      " 1    153791\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "X = df_balanced['clean_text']\n",
    "y = df_balanced['review_score']\n",
    "\n",
    "# Stratified split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Confirm shapes and class distribution\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1ee76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF training data shape: (1230331, 5000)\n",
      "TF-IDF test data shape: (307583, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,        # Use top 5000 words\n",
    "    ngram_range=(1, 2),       # Unigrams + bigrams\n",
    "    stop_words='english'      # Remove common English stop words\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both train and test sets\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Show result shape\n",
    "print(\"TF-IDF training data shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF test data shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575784a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8445817876800733\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.84      0.84    153792\n",
      "           1       0.84      0.85      0.85    153791\n",
      "\n",
      "    accuracy                           0.84    307583\n",
      "   macro avg       0.84      0.84      0.84    307583\n",
      "weighted avg       0.84      0.84      0.84    307583\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[129098  24694]\n",
      " [ 23110 130681]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize and train\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2536847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Accuracy: 0.8450727120809668\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.84      0.84    153792\n",
      "           1       0.84      0.85      0.85    153791\n",
      "\n",
      "    accuracy                           0.85    307583\n",
      "   macro avg       0.85      0.85      0.85    307583\n",
      "weighted avg       0.85      0.85      0.85    307583\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[128687  25105]\n",
      " [ 22548 131243]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train a LinearSVC model\n",
    "svc = LinearSVC(random_state=42)\n",
    "svc.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svc = svc.predict(X_test_tfidf)\n",
    "\n",
    "print(\"LinearSVC Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svc))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1695d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.8218562144201728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.83      0.82    153792\n",
      "           1       0.83      0.82      0.82    153791\n",
      "\n",
      "    accuracy                           0.82    307583\n",
      "   macro avg       0.82      0.82      0.82    307583\n",
      "weighted avg       0.82      0.82      0.82    307583\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[127438  26354]\n",
      " [ 28440 125351]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train MNB model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_mnb = mnb.predict(X_test_tfidf)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_mnb))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad87f94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Assign the best model explicitly\n",
    "best_model = LinearSVC()\n",
    "best_model.fit(X_train_tfidf, y_train)  # Re-train on the same data\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, \"linear_svc_model.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89c21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "\n",
    "# Load model and vectorizer\n",
    "model = joblib.load(\"linear_svc_model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Basic text cleaning function (same as used before)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(review):\n",
    "    cleaned = clean_text(review)\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    prediction = model.predict(vectorized)[0]\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fd707e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"This game is amazing! Loved every moment of it.\"))\n",
    "print(predict_sentiment(\"This is a terrible game. Waste of money.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ab9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Filter only positive reviews\n",
    "df_positive = df_balanced[df_balanced['review_score'] == 1]\n",
    "\n",
    "# Group positive reviews by app_name\n",
    "game_reviews = df_positive.groupby('app_name')['clean_text'].apply(lambda x: \" \".join(x)).reset_index()\n",
    "\n",
    "# Vectorize the combined review text per game\n",
    "tfidf_game = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "game_vectors = tfidf_game.fit_transform(game_reviews['clean_text'])\n",
    "\n",
    "# Compute cosine similarity matrix between games\n",
    "cosine_sim_matrix = cosine_similarity(game_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc527f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_games(game_name, top_n=3):\n",
    "    if game_name not in game_reviews['app_name'].values:\n",
    "        print(f\"Game '{game_name}' not found in the dataset.\")\n",
    "        return []\n",
    "\n",
    "    # Get index of the input game\n",
    "    idx = game_reviews[game_reviews['app_name'] == game_name].index[0]\n",
    "\n",
    "    # Get pairwise similarity scores and sort them\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Skip the first one (it's the input game itself)\n",
    "    sim_scores = sim_scores[1:top_n + 1]\n",
    "\n",
    "    # Get game names\n",
    "    similar_games = [game_reviews.iloc[i[0]]['app_name'] for i in sim_scores]\n",
    "    \n",
    "    print(f\"Because you liked **{game_name}**, you might also enjoy:\")\n",
    "    for game in similar_games:\n",
    "        print(\"→\", game)\n",
    "\n",
    "    return similar_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2f22ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because you liked **Terraria**, you might also enjoy:\n",
      "→ Magicite\n",
      "→ Crea\n",
      "→ Starbound\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Magicite', 'Crea', 'Starbound']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_similar_games(\"Terraria\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
